# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HC-e8D91cWEn22YqJM3ASlkZZUnD3jPL
"""

import pandas as pd
import os
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from keras.layers import Dense, Dropout
from keras.models import Sequential
import category_encoders as ce
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder , MinMaxScaler, OneHotEncoder
import seaborn as sns
from imblearn.over_sampling import SMOTE
import scipy.stats
import collections
from sklearn.decomposition import PCA
import statsmodels.api as sm


def create_model(optimizer='adam'):
    model2= Sequential()
    feature_length= len(X)
    #sgd = keras.optimizers.RMSprop(lr=.001)
    model2.add(Dense(30, input_dim= 22, kernel_initializer='he_normal',
                     activation='relu',
                     kernel_regularizer=l2(0.01)))
    model2.add(Dense(100,
                     kernel_regularizer=l2(0.01),
                     activation='sigmoid'))
    model2.add(Dense(200,
                     kernel_initializer='he_normal',
                     activation='relu',
                     kernel_regularizer=l2(0.01)))
    model2.add(Dense(400,
                     kernel_initializer='he_normal',
                     activation='relu',
                     kernel_regularizer=l2(0.01)))

    model2.add(Dense(1,  activation='sigmoid'))
    model2.compile(loss='binary_crossentropy', optimizer=optimizer,
                   metrics=['accuracy'])

    return model2

# Commented out IPython magic to ensure Python compatibility.
# %cd ../bin/kaggle_catgory_feature/

"""# New Section"""

df= pd.read_csv(r'train.csv')

df= df.sort_values(['bin_0'])

df= df.reset_index(drop='first')

for column in df.columns:
    df[column].fillna(df[column].mode()[0], inplace=True)

sm = SMOTE(random_state = 4)

for i in df.columns:
    print(i, " : ", df[i].value_counts())



#df= df.drop(['id', 'day', 'month' ], axis=1)
y= df['target']
df= df.drop(['target'], axis=1)
#df['month'].corr(y)


#Chi Square Testing
for i in df.columns:
    cont = pd.crosstab(df[i], y)
    chi= scipy.stats.chi2_contingency(cont)
    if chi[1]>0.1:
        print(chi[1])
        df= df.drop(i, axis=1)
cols= list(df.columns)




#df= df.fillna(0)
subset= df.select_dtypes(exclude=[np.number])

# =============================================================================
# for i in subset.columns:
#     if len(subset[i].value_counts())>=10:
#         value_dict= subset[i].value_counts().to_dict()
#         sorted_dict = {k: v for k, v in sorted(x.items(), key=lambda item: item[1])}
# =============================================================================
encoder= LabelEncoder()
item_list={}
for i in subset.columns:
    subset[i]= encoder.fit_transform(subset[i])
    #item_list[i]= subset[i].value_counts().to_dict()
    #subset[i]= subset[i].map(item_list[i])

df= df.select_dtypes(np.number)
for i in df.columns:
    print(df[i].corr(y))

df.corr()

df= pd.concat([df, subset],sort= False, axis=1)
X= df
m= sm.OLS(y,X).fit()
pca = PCA(n_components=2)
principalComponents = pca.fit_transform(X)
temp= pd.DataFrame(data= principalComponents)
temp['target']= y

plot= sns.pairplot(temp,  hue='target', diag_kind='hist')
X, y = sm.fit_sample(X, y.ravel())
scaler= MinMaxScaler()
X= scaler.fit_transform(X)
x_train,x_test,y_train,y_test= train_test_split(X, y, test_size= .2)


#Random Forest Classifier
model= RandomForestClassifier(n_estimators=1000)
model.fit(X, y)
model.score(x_test, y_test)


#XGBoost Classifier
model= xg_reg = xgb.XGBClassifier(subsample= 1.0,
                                 min_child_weight= 10,
                                 learning_rate= 0.1,
                                 gamma= 1.5,
                                 booster= 'gbtree',
                                 colsample_bytree= 1.0)
model.fit(X, y)
model.score(x_test, y_test)

import keras
from keras.models import Sequential
from keras.layers import Dense,Dropout
from keras.regularizers import l2
from sklearn.model_selection import GridSearchCV
from keras.wrappers.scikit_learn import KerasClassifier
import keras.optimizers


model = KerasClassifier(build_fn=create_model)
optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adam', 'Adamax', 'Nadam']
param_grid= dict(optimizer=optimizers)
grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)
grid_result = grid.fit(X, y)


#model2.fit(X, y, epochs=10, batch_size=10000)
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))
model2= create_model('SGD')
model2.fit(X,y, epochs=10)


#Test Data
df2= pd.read_csv(r'test.csv')
idcolumn= df2['id']
for column in df2.columns:
    df2[column].fillna(df2[column].mode()[0], inplace=True)
#df2= df2.drop(['id', 'day', 'month' ], axis=1)
df2= df2[cols]
#df2= df2.dropna(how='any')

subset2= df2.select_dtypes(exclude=[np.number])


encoder= LabelEncoder()
item_list={}
for i in subset2.columns:
    subset2[i]= encoder.fit_transform(subset2[i])
    #item_list[i]= subset2[i].value_counts().to_dict()
    #subset2[i]= subset2[i].map(item_list[i])



df2= df2.select_dtypes(np.number)

df2= pd.concat([df2, subset2],sort= False, axis=1)

scaler= MinMaxScaler()
#df2= scaler.fit_transform(df2)


predicted= model2.predict(df2)
for i in range(0,len(predicted)):
    if predicted[i][0]<0.5:
        predicted[i][0]=int(0)
    else:
        predicted[i][0]=int(1)

df3= pd.DataFrame(columns=['id','target'])

df3['id']= idcolumn
df3['target']= predicted
df3.to_csv('output.csv', index= False)
