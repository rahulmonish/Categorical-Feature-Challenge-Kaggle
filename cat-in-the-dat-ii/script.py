# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HC-e8D91cWEn22YqJM3ASlkZZUnD3jPL
"""

import pandas as pd
import os
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from keras.layers import Dense, Dropout
from keras.models import Sequential
import category_encoders as ce
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder , MinMaxScaler, OneHotEncoder
import seaborn as sns
from imblearn.over_sampling import SMOTE
import scipy.stats
import collections
from sklearn.decomposition import PCA




# Commented out IPython magic to ensure Python compatibility.
# %cd ../bin/kaggle_catgory_feature/

"""# New Section"""

df= pd.read_csv(r'train.csv')

for column in df.columns:
    df[column].fillna(df[column].mode()[0], inplace=True)

sm = SMOTE(random_state = 4)

# =============================================================================
# for i in df.columns:
#     print(i, " : ", df[i].value_counts())
# =============================================================================



#df= df.drop(['id', 'day', 'month' ], axis=1)
y= df['target']
df= df.drop(['target'], axis=1)
#df['month'].corr(y)


#Chi Square Testing
for i in df.columns:
    cont = pd.crosstab(df[i], y)
    chi= scipy.stats.chi2_contingency(cont)
    print(chi[1])
    if chi[1]<1.0:
        df= df.drop(i, axis=1)
cols= list(df.columns)




#df= df.fillna(0)
subset= df.select_dtypes(exclude=[np.number])




# =============================================================================
# for i in subset.columns:
#     if len(subset[i].value_counts())>=10:
#         value_dict= subset[i].value_counts().to_dict()
#         sorted_dict = {k: v for k, v in sorted(x.items(), key=lambda item: item[1])}
# =============================================================================








encoder= LabelEncoder()
item_list={}
for i in subset.columns:
    subset[i]= encoder.fit_transform(subset[i])
    #item_list[i]= subset[i].value_counts().to_dict()
    #subset[i]= subset[i].map(item_list[i])


df= df.select_dtypes(np.number)

df= pd.concat([df, subset],sort= False, axis=1)
X= df

# =============================================================================
# pca = PCA(n_components=2)
# principalComponents = pca.fit_transform(X)
# temp= pd.DataFrame(data= principalComponents)
# temp['target']= y
# =============================================================================

#sns.pairplot(temp,  hue='target', diag_kind='hist')


X, y = sm.fit_sample(X, y.ravel())


scaler= MinMaxScaler()
#X= scaler.fit_transform(X)

#x_train,x_test,y_train,y_test= train_test_split(X, y, test_size= .2)

#model= RandomForestClassifier(n_estimators=1000)






# =============================================================================
#
# model= xg_reg = xgb.XGBClassifier(subsample= 1.0,
#                                  min_child_weight= 10,
#                                  learning_rate= 0.1,
#                                  gamma= 1.5,
#                                  booster= 'gbtree',
#                                  colsample_bytree= 1.0)
# model.fit(X, y)
#
#
# model.score(x_test, y_test)
# =============================================================================








import keras
from keras.models import Sequential
from keras.layers import Dense,Dropout
from keras.regularizers import l2

model2= Sequential()
feature_length= len(X)
sgd = keras.optimizers.Adam(lr=.001)
model2.add(Dense(30, input_dim= 15, kernel_initializer='he_normal',
                 activation='relu',
                 kernel_regularizer=l2(0.01)))
model2.add(Dense(100,kernel_regularizer=l2(0.01), activation='sigmoid'))
model2.add(Dense(200,  kernel_initializer='he_normal', activation='relu',
                 kernel_regularizer=l2(0.01)))
model2.add(Dense(400,  kernel_initializer='he_normal', activation='relu',
                 kernel_regularizer=l2(0.01)))

model2.add(Dense(1,  activation='sigmoid'))
model2.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])

model2.fit(X, y, epochs=50, batch_size=10000)








#Test Data
df2= pd.read_csv(r'test.csv')
idcolumn= df2['id']
for column in df2.columns:
    df2[column].fillna(df2[column].mode()[0], inplace=True)
#df2= df2.drop(['id', 'day', 'month' ], axis=1)
df2= df2[cols]
#df2= df2.dropna(how='any')

subset2= df2.select_dtypes(exclude=[np.number])


encoder= LabelEncoder()
item_list={}
for i in subset2.columns:
    subset2[i]= encoder.fit_transform(subset2[i])
    #item_list[i]= subset2[i].value_counts().to_dict()
    #subset2[i]= subset2[i].map(item_list[i])



df2= df2.select_dtypes(np.number)

df2= pd.concat([df2, subset2],sort= False, axis=1)

scaler= MinMaxScaler()
#df2= scaler.fit_transform(df2)


predicted= model2.predict(df2)
for i in range(0,len(predicted)):
    if predicted[i][0]<0.5:
        predicted[i][0]=int(0)
    else:
        predicted[i][0]=int(1)

df3= pd.DataFrame(columns=['id','target'])

df3['id']= idcolumn
df3['target']= predicted
df3.to_csv('output.csv', index= False)
